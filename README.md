# vLLM Llama 3.2 API

ğŸš€ **Meta Llama 3.2 ëª¨ë¸ì„ ìœ„í•œ ê°„ì†Œí™”ëœ vLLM API ì„œë¹„ìŠ¤**

ì´ í”„ë¡œì íŠ¸ëŠ” vLLMì„ ì‚¬ìš©í•˜ì—¬ Meta Llama 3.2 ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì„œë¹™í•˜ëŠ” REST APIì…ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- ğŸ¤– **Llama 3.2 í…ìŠ¤íŠ¸ ìƒì„±** - ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ìƒì„± API
- ğŸ’¬ **ì±„íŒ… ì™„ì„±** - OpenAI í˜¸í™˜ ì±„íŒ… API
- âš–ï¸ **ë¡œë“œ ë°¸ëŸ°ì‹±** - ë‹¤ì¤‘ vLLM ì„œë²„ ì§€ì›
- ğŸ“Š **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§** - ì„œë²„ ìƒíƒœ ë° ì„±ëŠ¥ ì¶”ì 
- ğŸ”„ **í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬** - vLLM ì„œë²„ ìë™ ì‹œì‘/ì¤‘ì§€
- ğŸš¨ **ì•ŒëŸ¿ ì‹œìŠ¤í…œ** - ì¥ì•  ë° ì´ìƒ ìƒí™© ê°ì§€
- â¤ï¸ **í—¬ìŠ¤ ì²´í¬** - ì„œë¹„ìŠ¤ ê°€ìš©ì„± ëª¨ë‹ˆí„°ë§

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚    â”‚   Load Balancer â”‚    â”‚  vLLM Server 1  â”‚
â”‚                 â”‚â”€â”€â”€â–¶â”‚                 â”‚â”€â”€â”€â–¶â”‚  (Llama 3.2)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                 â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                 â”‚â”€â”€â”€â–¶â”‚  vLLM Server 2  â”‚
â”‚  Spring Boot    â”‚â”€â”€â”€â–¶â”‚                 â”‚    â”‚  (Llama 3.2)    â”‚
â”‚     API         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Monitoring    â”‚
                       â”‚   & Alerting    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- **Java 17+**
- **CUDA ì§€ì› GPU** (8GB+ VRAM ê¶Œì¥)
- **Python 3.8+** ë° vLLM
- **Git LFS** (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)
- **Hugging Face ê³„ì •** (Llama 3.2 ì•¡ì„¸ìŠ¤ìš©)

### 2ï¸âƒ£ í™˜ê²½ ì„¤ì •

```bash
# CUDA í™˜ê²½ í™•ì¸
nvidia-smi

# vLLM ì„¤ì¹˜
pip install vllm

# Hugging Face ë¡œê·¸ì¸ (Llama 3.2 ì•¡ì„¸ìŠ¤ìš©)
huggingface-cli login
```

```bash
# 8B ëª¨ë¸ (ê¶Œì¥)
huggingface-cli download torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1

### 4ï¸âƒ£ ì„¤ì • íŒŒì¼ ìˆ˜ì •

```yaml
# src/main/resources/application.yml
vllm:
  servers:
    - name: "llama32-primary"
      model: "torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì •
      host: "localhost"
      port: 8001
      enabled: true
```

### 5ï¸âƒ£ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

#### ë°©ë²• 1: Mavenìœ¼ë¡œ ì‹¤í–‰
```bash
# vLLM ì„œë²„ ì‹œì‘ (ë³„ë„ í„°ë¯¸ë„)
./docker/start-vllm.sh

# Spring Boot API ì‹œì‘
mvn spring-boot:run
```

#### ë°©ë²• 2: Docker Composeë¡œ ì‹¤í–‰
```bash
# ì „ì²´ ìŠ¤íƒ ì‹¤í–‰
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f
```

### 6ï¸âƒ£ API í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8080/health

# í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8080/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello, Llama 3.2! Explain quantum computing.",
    "temperature": 0.7,
    "maxTokens": 200
  }'
```

## ğŸ“– API ì‚¬ìš© ê°€ì´ë“œ

### ğŸ”¥ ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±

```bash
curl -X POST http://localhost:8080/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write a Python function to sort a list",
    "temperature": 0.3,
    "maxTokens": 500
  }'
```

### ğŸ’¬ ì±„íŒ… ì™„ì„± (OpenAI í˜¸í™˜)

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "You are a helpful coding assistant."},
      {"role": "user", "content": "How do I implement a binary search in Python?"}
    ],
    "temperature": 0.2,
    "maxTokens": 800
  }'
```

### ğŸ¯ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í™œìš©

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explain machine learning",
    "systemPrompt": "You are an expert data scientist. Explain concepts clearly with examples.",
    "temperature": 0.5,
    "maxTokens": 600
  }'
```

```

## âš™ï¸ ì„¤ì • ê°€ì´ë“œ

### ğŸ›ï¸ ëª¨ë¸ ì„¤ì •

```yaml
vllm:
  servers:
    - name: "llama32-1b"
      model: "torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"
      host: "localhost"
      port: 8001
      enabled: true
      model-settings:
        max-model-len: 8192        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (í† í°)
        max-num-seqs: 256          # ë™ì‹œ ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìˆ˜
        dtype: "auto"              # ìë™ ì •ë°€ë„ ì„ íƒ
        trust-remote-code: false   # ë³´ì•ˆì„ ìœ„í•´ false ê¶Œì¥
      performance-settings:
        gpu-memory-utilization: 0.9    # GPU ë©”ëª¨ë¦¬ 90% ì‚¬ìš©
        tensor-parallel-size: 1        # ë‹¨ì¼ GPUìš©
        disable-log-stats: false       # í†µê³„ ë¡œê¹… í™œì„±í™”
```

### ğŸ”§ ì„±ëŠ¥ ìµœì í™”

#### ë‹¨ì¼ GPU (RTX 4090, A100 ë“±)
```yaml
performance-settings:
  gpu-memory-utilization: 0.9
  tensor-parallel-size: 1
  max-num-seqs: 256
```

#### ë©€í‹° GPU (2x A100 ë“±)
```yaml
performance-settings:
  gpu-memory-utilization: 0.95
  tensor-parallel-size: 2
  max-num-seqs: 512
```

#### ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ (RTX 3080 ë“±)
```yaml
performance-settings:
  gpu-memory-utilization: 0.7
  tensor-parallel-size: 1
  max-num-seqs: 128
model-settings:
  max-model-len: 4096  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì„
```

### ğŸš¨ ì•ŒëŸ¿ ì„¤ì •

```yaml
# ê¸°ë³¸ ì•ŒëŸ¿ ê·œì¹™ (ìë™ ì„¤ì •ë¨)
- server_down: vLLM ì„œë²„ ë‹¤ìš´ ê°ì§€
- high_load: ë™ì‹œ ìš”ì²­ 50ê°œ ì´ˆê³¼
- memory_usage: GPU ë©”ëª¨ë¦¬ 95% ì´ˆê³¼
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ğŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

Spring Boot Actuatorë¥¼ í†µí•´ ë‹¤ìŒ ë©”íŠ¸ë¦­ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤:

```bash
# ëª¨ë“  ë©”íŠ¸ë¦­ ì¡°íšŒ
curl http://localhost:8080/actuator/metrics

# HTTP ìš”ì²­ ë©”íŠ¸ë¦­
curl http://localhost:8080/actuator/metrics/http.server.requests

# JVM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
curl http://localhost:8080/actuator/metrics/jvm.memory.used
```

### ğŸ” ë¡œê·¸ ëª¨ë‹ˆí„°ë§

```bash
# ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸
tail -f logs/vllm-llama32-api.log

# vLLM ì„œë²„ ë¡œê·¸ (Docker)
docker logs -f vllm-server

# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
nvidia-smi -l 1
```

## ğŸ³ Docker ë°°í¬

### ì „ì²´ ìŠ¤íƒ ë°°í¬
```bash
# í”„ë¡œë•ì…˜ í™˜ê²½
docker-compose -f docker-compose.prod.yml up -d

# ê°œë°œ í™˜ê²½
docker-compose up -d
```

### ê°œë³„ ì„œë¹„ìŠ¤ ë°°í¬
```bash
# API ì„œë²„ë§Œ
docker build -t vllm-llama32-api .
docker run -p 8080:8080 vllm-llama32-api

# vLLM ì„œë²„ë§Œ
docker run --gpus all -p 8001:8001 \
  -v ./models:/models \
  vllm/vllm-openai:latest \
  --model meta-llama/Meta-Llama-3.2-1B-Instruct \
  --host 0.0.0.0 --port 8001
```

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
src/main/java/com/yourcompany/llm/
â”œâ”€â”€ LlmApiApplication.java              # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ config/vllm/
â”‚   â””â”€â”€ VllmConfigProperties.java       # vLLM ì„¤ì •
â”œâ”€â”€ controller/
â”‚   â”œâ”€â”€ LlmController.java              # ê¸°ë³¸ LLM API
â”‚   â””â”€â”€ VllmController.java             # vLLM ê´€ë¦¬ API
â”œâ”€â”€ dto/
â”‚   â”œâ”€â”€ LlmRequest.java                 # ìš”ì²­ DTO
â”‚   â””â”€â”€ LlmResponse.java                # ì‘ë‹µ DTO
â”œâ”€â”€ service/
â”‚   â”œâ”€â”€ LlmService.java                 # ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ impl/LlmServiceImpl.java        # ì„œë¹„ìŠ¤ êµ¬í˜„
â”‚   â””â”€â”€ vllm/                           # vLLM ì „ìš© ì„œë¹„ìŠ¤ë“¤
â”‚       â”œâ”€â”€ VllmApiClient.java          # vLLM HTTP í´ë¼ì´ì–¸íŠ¸
â”‚       â”œâ”€â”€ VllmHealthChecker.java      # í—¬ìŠ¤ ì²´í¬
â”‚       â”œâ”€â”€ VllmLoadBalancer.java       # ë¡œë“œ ë°¸ëŸ°ì‹±
â”‚       â”œâ”€â”€ VllmMonitoringService.java  # ëª¨ë‹ˆí„°ë§
â”‚       â””â”€â”€ VllmProcessManager.java     # í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
```

### ğŸ”¨ ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸

```bash
# í”„ë¡œì íŠ¸ ë¹Œë“œ
mvn clean package

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
mvn test

# í†µí•© í…ŒìŠ¤íŠ¸ (vLLM ì„œë²„ í•„ìš”)
mvn integration-test
```

### ğŸ› ë””ë²„ê¹…

```bash
# ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
mvn spring-boot:run -Dspring-boot.run.jvmArguments="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005"

# ê°œë°œ í”„ë¡œíŒŒì¼ë¡œ ì‹¤í–‰
mvn spring-boot:run -Dspring.profiles.active=dev
```

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### â“ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. vLLM ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ
```bash
# ì›ì¸ í™•ì¸
nvidia-smi                    # GPU ìƒíƒœ í™•ì¸
docker logs vllm-server      # vLLM ë¡œê·¸ í™•ì¸
df -h                        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸

# í•´ê²°ë°©ë²•
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¤„ì´ê¸°
gpu-memory-utilization: 0.7  # 0.9 â†’ 0.7
```

#### 2. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```bash
# Hugging Face ì¸ì¦ í™•ì¸
huggingface-cli whoami

# ëª¨ë¸ ê²½ë¡œ í™•ì¸
ls -la ~/.cache/huggingface/hub/

# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
huggingface-cli download torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡± (CUDA OOM)
```yaml
# application.yml ìˆ˜ì •
performance-settings:
  gpu-memory-utilization: 0.6  # ë‚®ì¶¤
  max-num-seqs: 64             # ë°°ì¹˜ í¬ê¸° ì¤„ì„
model-settings:
  max-model-len: 4096          # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì„
```

#### 4. API ì‘ë‹µ ëŠë¦¼
```bash
# GPU ì‚¬ìš©ë¥  í™•ì¸
nvidia-smi

# ë„¤íŠ¸ì›Œí¬ ì§€ì—° í™•ì¸
curl -w "@curl-format.txt" http://localhost:8080/api/llm/health

# ìŠ¤ë ˆë“œ í’€ í¬ê¸° ì¡°ì • (application.yml)
server:
  tomcat:
    threads:
      max: 200
```
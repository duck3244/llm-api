# docker-compose.yml
version: '3.8'

services:
  vllm-llama31-api:
    build: .
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - JAVA_OPTS=-Xmx4g
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models  # 모델 파일 마운트
    depends_on:
      - vllm-server
    networks:
      - llama-network

  vllm-server:
    image: vllm/vllm-openai:latest
    ports:
      - "8001:8001"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./models:/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model meta-llama/Meta-Llama-3.1-8B-Instruct
      --host 0.0.0.0
      --port 8001
      --max-model-len 8192
      --gpu-memory-utilization 0.9
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - llama-network

networks:
  llama-network:
    driver: bridge
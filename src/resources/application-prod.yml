# application-prod.yml - 프로덕션 환경 설정
spring:
  config:
    activate:
      on-profile: prod

  # 데이터소스 설정 (프로덕션 DB)
  datasource:
    url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:llm_api_prod}
    username: ${DB_USERNAME:llm_user}
    password: ${DB_PASSWORD:}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      idle-timeout: 300000
      max-lifetime: 1200000
      connection-timeout: 20000
      validation-timeout: 5000
      leak-detection-threshold: 60000

  # JPA 설정 (프로덕션 최적화)
  jpa:
    hibernate:
      ddl-auto: validate  # 프로덕션에서는 스키마 변경 금지
    show-sql: false
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: false
        use_sql_comments: false
        jdbc:
          batch_size: 20
        order_inserts: true
        order_updates: true
        generate_statistics: false
        cache:
          use_second_level_cache: true
          use_query_cache: true
          region:
            factory_class: org.hibernate.cache.jcache.JCacheRegionFactory

  # Redis 설정 (프로덕션)
  data:
    redis:
      host: ${REDIS_HOST:localhost}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}
      database: 0
      timeout: 2000ms
      jedis:
        pool:
          max-active: 50
          max-idle: 20
          min-idle: 5
          max-wait: 3000ms

  # 캐시 설정
  cache:
    type: redis
    redis:
      time-to-live: 3600000  # 1시간
      cache-null-values: false
      use-key-prefix: true
      key-prefix: "llm:prod:"

  # Jackson 설정
  jackson:
    serialization:
      write-dates-as-timestamps: false
      indent-output: false
    deserialization:
      fail-on-unknown-properties: false
    default-property-inclusion: non_null

# 서버 설정 (프로덕션)
server:
  port: ${SERVER_PORT:8080}
  servlet:
    context-path: ${CONTEXT_PATH:}
  compression:
    enabled: true
    mime-types: application/json,application/xml,text/html,text/xml,text/plain,text/css,application/javascript
    min-response-size: 1024
  http2:
    enabled: true
  error:
    include-exception: false
    include-stacktrace: never
    include-message: always

# LLM 설정 (프로덕션)
llm:
  models:
    # OpenAI GPT-4 (프로덕션 메인)
    - name: "gpt-4"
      provider: "openai"
      endpoint: "https://api.openai.com/v1/chat/completions"
      apiKey: "${OPENAI_API_KEY:}"
      maxTokens: 4096
      temperature: 0.7
      enabled: true
      limits:
        maxRequestsPerMinute: 500
        maxTokensPerMinute: 400000
        maxConcurrentRequests: 50
        costPerInputToken: 0.00003
        costPerOutputToken: 0.00006
      features:
        supportsStreaming: true
        supportsFunctionCalling: true
        specializations: ["reasoning", "analysis", "general"]

    # OpenAI GPT-3.5 (백업 및 저비용)
    - name: "gpt-3.5-turbo"
      provider: "openai"
      endpoint: "https://api.openai.com/v1/chat/completions"
      apiKey: "${OPENAI_API_KEY:}"
      maxTokens: 4096
      temperature: 0.7
      enabled: true
      limits:
        maxRequestsPerMinute: 3500
        maxTokensPerMinute: 900000
        maxConcurrentRequests: 100
        costPerInputToken: 0.0015
        costPerOutputToken: 0.002
      features:
        supportsStreaming: true
        supportsFunctionCalling: true
        specializations: ["conversation", "general"]

    # Anthropic Claude (대안)
    - name: "claude-3"
      provider: "anthropic"
      endpoint: "https://api.anthropic.com/v1/messages"
      apiKey: "${ANTHROPIC_API_KEY:}"
      maxTokens: 4096
      temperature: 0.7
      enabled: ${CLAUDE_ENABLED:false}
      limits:
        maxRequestsPerMinute: 1000
        maxTokensPerMinute: 400000
        maxConcurrentRequests: 20
        costPerInputToken: 0.015
        costPerOutputToken: 0.075
      features:
        supportsStreaming: true
        supportsFunctionCalling: false
        specializations: ["reasoning", "analysis", "creative"]

    # vLLM Llama3 (고성능 오픈소스)
    - name: "llama3-prod"
      provider: "vllm"
      endpoint: "http://${VLLM_HOST:localhost}:${VLLM_PORT:8000}/v1/chat/completions"
      model: "meta-llama/Meta-Llama-3-8B-Instruct"
      maxTokens: 4096
      temperature: 0.7
      enabled: ${VLLM_ENABLED:true}
      limits:
        maxRequestsPerMinute: 2000
        maxTokensPerMinute: 1000000
        maxConcurrentRequests: 200
        costPerInputToken: 0.0  # 자체 호스팅
        costPerOutputToken: 0.0
      features:
        supportsStreaming: false
        supportsFunctionCalling: false
        specializations: ["conversation", "coding", "general"]

  defaults:
    model: "gpt-3.5-turbo"  # 기본은 비용 효율적인 모델
    timeout: 60000  # 60초 (프로덕션에서 더 긴 타임아웃)
    retryAttempts: 5
    retryDelayMs: 2000
    retryMultiplier: 2.0
    enableCircuitBreaker: true
    enableMetrics: true
    enableCaching: true

  retry:
    maxAttempts: 5
    initialDelayMs: 2000
    multiplier: 2.0
    maxDelayMs: 60000
    enableJitter: true
    retryableErrors:
      - "TIMEOUT"
      - "CONNECTION_ERROR"
      - "RATE_LIMIT"
      - "SERVER_ERROR"
      - "SERVICE_UNAVAILABLE"

  security:
    enableApiKeyValidation: true
    enableRateLimiting: true
    enableRequestLogging: false  # 프로덕션에서는 성능상 비활성화
    enableResponseLogging: false
    maskSensitiveData: true
    maxRequestSizeKb: 2048  # 2MB
    requestsPerMinute: 1000
    allowedOrigins:
      - "https://${FRONTEND_DOMAIN:example.com}"
      - "https://api.${FRONTEND_DOMAIN:example.com}"
    sensitiveFields:
      - "apiKey"
      - "password"
      - "token"
      - "secret"
      - "authorization"

  monitoring:
    enableHealthChecks: true
    enableMetrics: true
    enableAlerts: true
    enablePerformanceTracking: true
    healthCheckIntervalSeconds: 60
    metricsIntervalSeconds: 30
    alertCheckIntervalSeconds: 60
    alertThresholds:
      errorRateThreshold: 0.05  # 5%
      responseTimeThresholdMs: 10000  # 10초
      requestVolumeThreshold: 10000
      cacheHitRateThreshold: 0.7  # 70%
      cpuUsageThreshold: 0.8
      memoryUsageThreshold: 0.85

  cache:
    enableResponseCaching: true
    enableModelInfoCaching: true
    enableConfigCaching: true
    defaultTtlSeconds: 7200  # 2시간
    responseCacheTtlSeconds: 86400  # 24시간
    modelInfoCacheTtlSeconds: 3600  # 1시간
    configCacheTtlSeconds: 1800  # 30분
    maxCacheSizeMb: 1024  # 1GB
    enableCacheMetrics: true
    enableCacheCompression: true

# vLLM 설정 (프로덕션)
vllm:
  servers:
    - name: "llama3-prod-primary"
      model: "meta-llama/Meta-Llama-3-8B-Instruct"
      port: 8000
      host: "${VLLM_HOST:0.0.0.0}"
      enabled: true
      modelSettings:
        maxModelLen: 8192
        maxNumSeqs: 512
        dtype: "auto"
        trustRemoteCode: false
      performanceSettings:
        gpuMemoryUtilization: 0.95
        tensorParallelSize: ${VLLM_TP_SIZE:2}
        pipelineParallelSize: 1
        disableLogStats: true
      quantizationSettings:
        quantization: "awq"  # 프로덕션에서 메모리 최적화
        loadFormat: "auto"
        enforceEager: false

    - name: "llama3-prod-secondary"
      model: "meta-llama/Meta-Llama-3-8B-Instruct"
      port: 8001
      host: "${VLLM_HOST:0.0.0.0}"
      enabled: ${VLLM_SECONDARY_ENABLED:false}
      modelSettings:
        maxModelLen: 8192
        maxNumSeqs: 512
        dtype: "auto"
      performanceSettings:
        gpuMemoryUtilization: 0.95
        tensorParallelSize: ${VLLM_TP_SIZE:2}
        pipelineParallelSize: 1
        disableLogStats: true
      quantizationSettings:
        quantization: "awq"
        loadFormat: "auto"

  globalSettings:
    seed: 42
    disableLogRequests: true  # 프로덕션에서 로그 최소화
    logLevel: "WARNING"
    requestTimeout: 120000  # 2분
    enableMetrics: true

  resourceSettings:
    device: "cuda"
    gpuIds: [0, 1]
    numGpus: ${VLLM_NUM_GPUS:2}

  securitySettings:
    apiKey: "${VLLM_API_KEY:}"
    corsEnabled: false  # 프로덕션에서는 CORS 비활성화
    allowedOrigins: []

# 액추에이터 설정 (프로덕션)
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
      base-path: /actuator
  endpoint:
    health:
      show-details: when_authorized
      show-components: when_authorized
      probes:
        enabled: true
    metrics:
      enabled: true
    prometheus:
      enabled: true
  metrics:
    export:
      prometheus:
        enabled: true
        step: 30s
    distribution:
      percentiles-histogram:
        http.server.requests: true
      percentiles:
        http.server.requests: 0.5, 0.95, 0.99
    tags:
      environment: production
      service: llm-api

# 로깅 설정 (프로덕션)
logging:
  level:
    root: WARN
    com.yourcompany.llm: INFO
    org.springframework.web: WARN
    org.hibernate: WARN
    org.springframework.cache: WARN
    org.springframework.data.redis: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level [%X{traceId},%X{spanId}] %logger{36} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level [%X{traceId},%X{spanId}] %logger{36} - %msg%n"
  file:
    name: /var/log/llm-api/application.log
    max-size: 100MB
    max-history: 30
    total-size-cap: 3GB

# CORS 설정 (프로덕션)
cors:
  allowed-origins:
    - "https://${FRONTEND_DOMAIN:example.com}"
    - "https://api.${FRONTEND_DOMAIN:example.com}"
  allowed-methods:
    - GET
    - POST
    - PUT
    - DELETE
    - OPTIONS
  allowed-headers:
    - Authorization
    - Content-Type
    - X-Requested-With
    - X-API-Key
  allow-credentials: true
  max-age: 3600

# SSL/TLS 설정
server:
  ssl:
    enabled: ${SSL_ENABLED:false}
    key-store: ${SSL_KEYSTORE_PATH:}
    key-store-password: ${SSL_KEYSTORE_PASSWORD:}
    key-store-type: PKCS12
    protocol: TLS
    enabled-protocols: TLSv1.2,TLSv1.3

# 환경별 외부 설정
external:
  api:
    timeout:
      connect: 10000
      read: 60000
    retry:
      maxAttempts: 3
      backoffMultiplier: 2.0
  
  monitoring:
    alerting:
      enabled: true
      webhookUrl: ${ALERT_WEBHOOK_URL:}
      slackChannel: ${SLACK_CHANNEL:#alerts}
    
    tracing:
      enabled: ${TRACING_ENABLED:true}
      samplingRate: 0.1  # 10% 샘플링 (성능 고려)
      jaegerEndpoint: ${JAEGER_ENDPOINT:http://localhost:14268/api/traces}

# 성능 튜닝 설정
performance:
  threadPool:
    coreSize: 20
    maxSize: 100
    queueCapacity: 1000
    keepAlive: 60
  
  cache:
    caffeine:
      maximumSize: 10000
      expireAfterWrite: 3600
      expireAfterAccess: 1800
  
  rateLimit:
    global:
      requestsPerSecond: 1000
      burstCapacity: 2000
    perUser:
      requestsPerMinute: 100
      burstCapacity: 200

# 보안 설정
security:
  headers:
    frameOptions: DENY
    contentTypeOptions: true
    xssProtection: true
    referrerPolicy: strict-origin-when-cross-origin
    permissionsPolicy: "geolocation=(), microphone=(), camera=()"

# JVM 메모리 설정 가이드 (주석)
# -Xms2g -Xmx8g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 
# -XX:+UseStringDeduplication -XX:+OptimizeStringConcat